{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First initial libraries. Machine learning has been left out for now.\n",
    "\n",
    "Additionally, the datasets are much too large for my laptops memory, so I will load truncated versions. I will also keep a commented-out cell which attempts to either load the whole dataset, or use a SQL server for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data analysis and wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "import nltk\n",
    "import math\n",
    "from sqlalchemy import create_engine\n",
    "#nltk.download()\n",
    "\n",
    "# visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first cell here will upload smaller sections of the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "raw_2016 = pd.read_csv('properties_2016.csv', nrows = 30000)\n",
    "raw_2017 = pd.read_csv('properties_2017.csv', nrows = 30000)\n",
    "vals_2016 = pd.read_csv('train_2016_v2.csv')\n",
    "vals_2017 = pd.read_csv('train_2017.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will load the entire datasets, and I will likely not ever be able to use this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw_2016 = pd.read_csv('properties_2016.csv', low_memory = False)\n",
    "#raw_2017 = pd.read_csv('properties_2017.csv', low_memory = False)\n",
    "#vals_2016 = pd.read_csv('train_2016_v2.csv')\n",
    "#vals_2017 = pd.read_csv('train_2017.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally a third possible solution that lets me use SQL to pull in large datasets after I have figured out how to trim the dataset.\n",
    "\n",
    "This still has big memory issues, so it may not end up being used either. I would also like to know if there is a way to minimize a cell as this the SQL cell takes up a lot of room."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "vals_2016 = pd.read_csv('train_2016_v2.csv')\n",
    "vals_2017 = pd.read_csv('train_2017.csv')\n",
    "\n",
    "csv_database = create_engine('sqlite:///csv_database.db')\n",
    "\n",
    "# Next we load a the 2016 dataset into a SQL engine table chunk by chunk\n",
    "chunksize = 10000\n",
    "i_1 = 0\n",
    "j_1 = 1\n",
    "for df in pd.read_csv('properties_2016.csv', chunksize=chunksize, iterator=True):\n",
    "      df = df.rename(columns={c: c.replace(' ', '') for c in df.columns}) \n",
    "      df.index += j_1\n",
    "      i_1+=1\n",
    "      df.to_sql('table_2016', csv_database, if_exists='append')\n",
    "      j = df.index[-1] + 1\n",
    "#now we do the same for 2017 into another table\n",
    "i_2 = 0\n",
    "j_2 = 1\n",
    "for df in pd.read_csv('properties_2017.csv', chunksize=chunksize, iterator=True):\n",
    "      df = df.rename(columns={c: c.replace(' ', '') for c in df.columns}) \n",
    "      df.index += j_2\n",
    "      i_2+=1\n",
    "      df.to_sql('table_2017', csv_database, if_exists='append')\n",
    "      j = df.index[-1] + 1\n",
    "        \n",
    "df = pd.read_sql_query('SELECT COl1 table_2017', csv_database)\n",
    "df.head()\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can merge the dataframes with their vals we need to see how long each are. We note that there are far more in the vals dataframe, but since we did not truncate those csv files but did for the raw files, we do not know if vals is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 90275)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_2016),len(vals_2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will merge the raw df with the vals df. However, we note that the df's when merged on 'inner' are MUCH smaller than the starting ones, meaning that the train csv's were smaller than the raw csv's. However, since this notebook is primary concerned with munging and cleaning, we will proceed anyway, hoping for a solution where we can deal with the larger datasets.\n",
    "\n",
    "Another important observation is that the mergeing process gained us two extra rows so we no longer have unique entries in parcelid. We will need to look at how into that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(896, 894)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2016 = pd.merge(raw_2016, vals_2016, how='inner', left_on='parcelid', right_on='parcelid')\n",
    "df2017 = pd.merge(raw_2017, vals_2017, how='inner', left_on='parcelid', right_on='parcelid')\n",
    "len(df2016),len(df2016['parcelid'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets get some information on the structure of these dataframes as well as what types of data are inside it. The biggest observations we can make here is that there are many empty columns, likely resulting from the extreme truncation. As a result, we will need merge with how='left'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 896 entries, 0 to 895\n",
      "Data columns (total 60 columns):\n",
      "parcelid                        896 non-null int64\n",
      "airconditioningtypeid           276 non-null float64\n",
      "architecturalstyletypeid        0 non-null float64\n",
      "basementsqft                    0 non-null float64\n",
      "bathroomcnt                     896 non-null float64\n",
      "bedroomcnt                      896 non-null float64\n",
      "buildingclasstypeid             0 non-null float64\n",
      "buildingqualitytypeid           551 non-null float64\n",
      "calculatedbathnbr               884 non-null float64\n",
      "decktypeid                      7 non-null float64\n",
      "finishedfloor1squarefeet        67 non-null float64\n",
      "calculatedfinishedsquarefeet    890 non-null float64\n",
      "finishedsquarefeet12            851 non-null float64\n",
      "finishedsquarefeet13            0 non-null float64\n",
      "finishedsquarefeet15            35 non-null float64\n",
      "finishedsquarefeet50            67 non-null float64\n",
      "finishedsquarefeet6             4 non-null float64\n",
      "fips                            896 non-null int64\n",
      "fireplacecnt                    97 non-null float64\n",
      "fullbathcnt                     884 non-null float64\n",
      "garagecarcnt                    314 non-null float64\n",
      "garagetotalsqft                 314 non-null float64\n",
      "hashottuborspa                  21 non-null object\n",
      "heatingorsystemtypeid           536 non-null float64\n",
      "latitude                        896 non-null int64\n",
      "longitude                       896 non-null int64\n",
      "lotsizesquarefeet               781 non-null float64\n",
      "poolcnt                         183 non-null float64\n",
      "poolsizesum                     9 non-null float64\n",
      "pooltypeid10                    7 non-null float64\n",
      "pooltypeid2                     14 non-null float64\n",
      "pooltypeid7                     169 non-null float64\n",
      "propertycountylandusecode       896 non-null object\n",
      "propertylandusetypeid           896 non-null int64\n",
      "propertyzoningdesc              564 non-null object\n",
      "rawcensustractandblock          896 non-null float64\n",
      "regionidcity                    876 non-null float64\n",
      "regionidcounty                  896 non-null int64\n",
      "regionidneighborhood            372 non-null float64\n",
      "regionidzip                     895 non-null float64\n",
      "roomcnt                         896 non-null float64\n",
      "storytypeid                     0 non-null float64\n",
      "threequarterbathnbr             136 non-null float64\n",
      "typeconstructiontypeid          0 non-null float64\n",
      "unitcnt                         560 non-null float64\n",
      "yardbuildingsqft17              26 non-null float64\n",
      "yardbuildingsqft26              0 non-null float64\n",
      "yearbuilt                       888 non-null float64\n",
      "numberofstories                 196 non-null float64\n",
      "fireplaceflag                   0 non-null object\n",
      "structuretaxvaluedollarcnt      892 non-null float64\n",
      "taxvaluedollarcnt               896 non-null float64\n",
      "assessmentyear                  896 non-null int64\n",
      "landtaxvaluedollarcnt           896 non-null float64\n",
      "taxamount                       896 non-null float64\n",
      "taxdelinquencyflag              13 non-null object\n",
      "taxdelinquencyyear              13 non-null float64\n",
      "censustractandblock             889 non-null float64\n",
      "logerror                        896 non-null float64\n",
      "transactiondate                 896 non-null object\n",
      "dtypes: float64(47), int64(7), object(6)\n",
      "memory usage: 427.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df2016.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
